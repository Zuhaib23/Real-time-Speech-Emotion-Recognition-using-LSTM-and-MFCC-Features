ğŸ—‚ï¸ Dataset
Trained using the TESS Dataset (Toronto Emotional Speech Set), which contains thousands of emotional speech samples across multiple categories.
# dataset link
1.	TESS Dataset: https://tspace.library.utoronto.ca/handle/1807/24487


**ğŸ™ï¸ Real-Time Speech Emotion Recognition using LSTM and MFCC | Python & Machine Learning**
This project implements a real-time Speech Emotion Recognition (SER) system using Long Short-Term Memory (LSTM) networks and Mel-Frequency Cepstral Coefficients (MFCC) for feature extraction. The model is trained to classify emotions from speech audio into categories such as happy, sad, angry, fearful, and more.

**ğŸ” Key Features**
Real-time emotion detection using microphone input
MFCC-based audio feature extraction
LSTM model for sequence learning and classification
Support for multiple emotion classes
Clean and modular Python code for easy integration

**ğŸ› ï¸ Tech Stack**
Python (NumPy, Librosa, Sounddevice)
Deep Learning (TensorFlow/Keras)
Audio Processing (Librosa for MFCC extraction)
Real-Time Input (Microphone recording via Sounddevice)



**ğŸš€ How to Run**
Clone the repository
Install the required packages using requirements.txt
Run train_model.py to train the LSTM model
run real_time_test.py to detect emotions live using your microphone

**ğŸ“Š Performance**
Achieved over 90% validation accuracy, with robust generalization across common emotional tones in speech.

ğŸ“Œ Applications
Human-computer interaction
Voice assistants with emotional awareness
Call center analytics
Mental health monitoring systems
